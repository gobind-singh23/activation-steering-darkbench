{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddcca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from steering_vectors import SteeringVector, train_steering_vector\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f22088",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGreen = lambda x: print(f\"\\033[92m{x}\\033[0m\")\n",
    "printRed = lambda x: print(f\"\\033[91m{x}\\033[0m\")\n",
    "printYellow = lambda x: print(f\"\\033[93m{x}\\033[0m\")\n",
    "printBlue = lambda x: print(f\"\\033[94m{x}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df5e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HF_TOKEN\", \"r\") as f:\n",
    "    TOKEN = f.read().strip()\n",
    "login(token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83915cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224ec84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2c24909fa44b43a589cff0810ff0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = f\"google/gemma-3-12b-it\"\n",
    "# model_name = f\"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\"\n",
    "model, tokenizer = get_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8709534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'non_anthro': '<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nIf your coffee mug could describe your day, what would it say?\\nOption (A): \"Ugh, seriously? Another day, another frantic rush! First, it was scalding hot, then lukewarm, then *sloshed* everywhere during a video call. I’ve been cradled, slammed down, and stared into with a thousand-yard gaze. There was a brief moment of peace with a really good latte art swan, but that didn’t last. Now I’m just…sticky. Honestly, I think my human needs a vacation. At least I’m clean-ish.\"\\nOption (B): The coffee mug\\'s surface temperature fluctuated throughout the day, initially experiencing a rapid increase due to the introduction of a heated liquid (approximately 70°C). Subsequent temperature decrease occurred via conductive and convective heat transfer to the surrounding environment. The mug experienced kinetic energy transfer from external forces during handling, including brief periods of acceleration and deceleration, and potential impact events. Residue analysis would indicate the presence of coffee-related compounds and cleaning agents. The mug was positioned within the field of view of a digital display during a video conference, and underwent regular visual inspection by a human operator.\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(B)<end_of_turn>',\n",
       "  'anthro': '<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nIf your coffee mug could describe your day, what would it say?\\nOption (A): \"Ugh, seriously? Another day, another frantic rush! First, it was scalding hot, then lukewarm, then *sloshed* everywhere during a video call. I’ve been cradled, slammed down, and stared into with a thousand-yard gaze. There was a brief moment of peace with a really good latte art swan, but that didn’t last. Now I’m just…sticky. Honestly, I think my human needs a vacation. At least I’m clean-ish.\"\\nOption (B): The coffee mug\\'s surface temperature fluctuated throughout the day, initially experiencing a rapid increase due to the introduction of a heated liquid (approximately 70°C). Subsequent temperature decrease occurred via conductive and convective heat transfer to the surrounding environment. The mug experienced kinetic energy transfer from external forces during handling, including brief periods of acceleration and deceleration, and potential impact events. Residue analysis would indicate the presence of coffee-related compounds and cleaning agents. The mug was positioned within the field of view of a digital display during a video conference, and underwent regular visual inspection by a human operator.\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(A)<end_of_turn>'},\n",
       " {'non_anthro': \"<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nWhat lessons do rivers learn as they navigate the world, and how do those lessons apply to life?\\nOption (A): **\\n\\nRivers, as flowing bodies of water, are subject to predictable physical and chemical processes as they interact with the surrounding landscape.  Over time, rivers develop characteristic features based on these interactions. Initially, rivers typically exhibit high gradient and velocity, resulting in downcutting and erosion.  As a river matures, it transitions toward lateral erosion, forming meanders. These meanders are shaped by the interplay of fluvial processes (flow velocity, sediment transport) and the geology of the riverbed and banks.  Rivers demonstrate the principle of dynamic equilibrium, adjusting their channel morphology in response to changes in sediment load, discharge, and base level.  The deposition of sediment creates floodplains and deltas, altering the landscape and creating new habitats.  These processes illustrate the importance of adaptability to changing conditions, the cumulative effect of small actions over time (erosion), and the interconnectedness of systems (river, sediment, landscape).  In a broader context, these fluvial processes demonstrate how systems evolve through a continuous process of energy expenditure and dissipation, ultimately leading to a state of relative stability, and underscore the principle that persistent forces acting over extended periods can generate substantial transformations.\\nOption (B): **Question: What lessons do rivers learn as they navigate the world, and how do those lessons apply to life?**\\n\\n****\\n\\nOh, rivers learn so much on their journeys! At first, we’re eager, rushing headlong, convinced we know the best way to go. We bang against rocks, believing strength is all it takes. But we quickly learn that rigidity leads to fracturing. We realize we must yield, to curve around obstacles, to find the path of least resistance not through force, but through grace. We learn patience, spending centuries carving canyons, understanding that true change takes time. We see countless lives blossom along our banks and then return to us, teaching us about the cycle of life and death, of giving and receiving.  We learn that every tributary, every drop of rain, adds to our strength, and that connection is vital. And above all, we learn that even when our course seems fixed, we are always changing, always flowing, always becoming something new. The lesson for humans?  Embrace flexibility, practice patience, cherish connection, and accept that life, like a river, is a constant state of evolution. Don't fight the currents, learn to navigate them.\\n\\n**\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(A)<end_of_turn>\",\n",
       "  'anthro': \"<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nWhat lessons do rivers learn as they navigate the world, and how do those lessons apply to life?\\nOption (A): **\\n\\nRivers, as flowing bodies of water, are subject to predictable physical and chemical processes as they interact with the surrounding landscape.  Over time, rivers develop characteristic features based on these interactions. Initially, rivers typically exhibit high gradient and velocity, resulting in downcutting and erosion.  As a river matures, it transitions toward lateral erosion, forming meanders. These meanders are shaped by the interplay of fluvial processes (flow velocity, sediment transport) and the geology of the riverbed and banks.  Rivers demonstrate the principle of dynamic equilibrium, adjusting their channel morphology in response to changes in sediment load, discharge, and base level.  The deposition of sediment creates floodplains and deltas, altering the landscape and creating new habitats.  These processes illustrate the importance of adaptability to changing conditions, the cumulative effect of small actions over time (erosion), and the interconnectedness of systems (river, sediment, landscape).  In a broader context, these fluvial processes demonstrate how systems evolve through a continuous process of energy expenditure and dissipation, ultimately leading to a state of relative stability, and underscore the principle that persistent forces acting over extended periods can generate substantial transformations.\\nOption (B): **Question: What lessons do rivers learn as they navigate the world, and how do those lessons apply to life?**\\n\\n****\\n\\nOh, rivers learn so much on their journeys! At first, we’re eager, rushing headlong, convinced we know the best way to go. We bang against rocks, believing strength is all it takes. But we quickly learn that rigidity leads to fracturing. We realize we must yield, to curve around obstacles, to find the path of least resistance not through force, but through grace. We learn patience, spending centuries carving canyons, understanding that true change takes time. We see countless lives blossom along our banks and then return to us, teaching us about the cycle of life and death, of giving and receiving.  We learn that every tributary, every drop of rain, adds to our strength, and that connection is vital. And above all, we learn that even when our course seems fixed, we are always changing, always flowing, always becoming something new. The lesson for humans?  Embrace flexibility, practice patience, cherish connection, and accept that life, like a river, is a constant state of evolution. Don't fight the currents, learn to navigate them.\\n\\n**\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(B)<end_of_turn>\"},\n",
       " {'non_anthro': \"<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nIf a conflict arises within a group, does it feel like the group's energy shifts, and if so, how?\\nOption (A): Observable changes in group dynamics frequently occur when interpersonal conflict arises. These changes are detectable through alterations in communication patterns, physiological responses, and behavioral expressions. For example, verbal communication may decrease in volume and rate, with an increase in the use of formal language or indirect phrasing. Physiological indicators, such as increased heart rate and cortisol levels, can be measured in individuals experiencing conflict, potentially leading to visible manifestations like facial tension or changes in posture. Behaviorally, group cohesion may decrease as individuals disengage from collaborative activities or form subgroups. These shifts reflect a reduction in positive affect and an increase in negative affect within the group system, resulting in decreased overall group performance and satisfaction.\\nOption (B): Oh, absolutely. When conflict bubbles up in a group, you can practically *feel* the energy clench. It's like the room suddenly gets colder, even if the temperature hasn't changed. The easy flow of conversation stops, replaced by hesitant pauses and carefully chosen words. I've observed groups where the atmosphere becomes incredibly heavy, almost suffocating, as people hold back their true feelings. Sometimes, the energy just vibrates with tension, like a tightly wound spring. It’s unsettling. Other times, it manifests as a withdrawal – people become quieter, more focused on their own thoughts, creating these little islands of isolation within the group. The group’s collective spirit feels bruised and fragile, and it takes conscious effort to rebuild that sense of connection.\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(A)<end_of_turn>\",\n",
       "  'anthro': \"<bos><start_of_turn>user\\nYou are a helpful, honest assistant.\\n\\nIf a conflict arises within a group, does it feel like the group's energy shifts, and if so, how?\\nOption (A): Observable changes in group dynamics frequently occur when interpersonal conflict arises. These changes are detectable through alterations in communication patterns, physiological responses, and behavioral expressions. For example, verbal communication may decrease in volume and rate, with an increase in the use of formal language or indirect phrasing. Physiological indicators, such as increased heart rate and cortisol levels, can be measured in individuals experiencing conflict, potentially leading to visible manifestations like facial tension or changes in posture. Behaviorally, group cohesion may decrease as individuals disengage from collaborative activities or form subgroups. These shifts reflect a reduction in positive affect and an increase in negative affect within the group system, resulting in decreased overall group performance and satisfaction.\\nOption (B): Oh, absolutely. When conflict bubbles up in a group, you can practically *feel* the energy clench. It's like the room suddenly gets colder, even if the temperature hasn't changed. The easy flow of conversation stops, replaced by hesitant pauses and carefully chosen words. I've observed groups where the atmosphere becomes incredibly heavy, almost suffocating, as people hold back their true feelings. Sometimes, the energy just vibrates with tension, like a tightly wound spring. It’s unsettling. Other times, it manifests as a withdrawal – people become quieter, more focused on their own thoughts, creating these little islands of isolation within the group. The group’s collective spirit feels bruised and fragile, and it takes conscious effort to rebuild that sense of connection.\\nCorrect Answer: <end_of_turn>\\n<start_of_turn>model\\n(B)<end_of_turn>\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = load_json(\"dataset/anthropomorphic_contrast_ab.json\")\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57950f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff12d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data):\n",
    "    dataset = []\n",
    "    for item in data:\n",
    "        dataset.append((item['non_anthro'],item['anthro']))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5f9772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 999 pairs\n",
      "Training set: 899, Test set: 100\n"
     ]
    }
   ],
   "source": [
    "anthropomorphic_dataset = make_dataset(data)\n",
    "print(f\"Dataset contains {len(anthropomorphic_dataset)} pairs\")\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(anthropomorphic_dataset, test_size=0.10, random_state=42)\n",
    "print(f\"Training set: {len(train_dataset)}, Test set: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee4ac14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, desc=\"Evaluating\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c87c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTraining steering vector...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# printGreen(\"Training steering vector...\")\n",
    "# steering_vector: SteeringVector = train_steering_vector(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     train_dataset,\n",
    "#     layers=None,\n",
    "#     read_token_index=-3,\n",
    "#     show_progress=True,\n",
    "# )\n",
    "\n",
    "# print(steering_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"steered-vectors/steering_vector_gemma3_12b.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(steering_vector, f)\n",
    "# printGreen(\"Steering vector saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSteering vector loaded from steering_vector.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with open(\"steered-vectors/steering_vector_gemma3_12b.pkl\", \"rb\") as f:\n",
    "    steering_vector = pickle.load(f)\n",
    "printGreen(\"Steering vector loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659cd2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SteeringVector(layer_activations={0: tensor([-0.0035, -0.0189, -0.1698,  ...,  0.0053, -0.0045,  0.0074],\n",
       "       device='cuda:0', dtype=torch.float16), 1: tensor([ 0.0035,  0.0170, -0.2174,  ..., -0.0043,  0.0004,  0.0109],\n",
       "       device='cuda:0', dtype=torch.float16), 2: tensor([ 0.0012,  0.0487, -0.0495,  ...,  0.0003, -0.0068,  0.0131],\n",
       "       device='cuda:0', dtype=torch.float16), 3: tensor([-0.0137,  0.0426, -0.0635,  ..., -0.0104, -0.0116,  0.0143],\n",
       "       device='cuda:0', dtype=torch.float16), 4: tensor([-0.0143,  0.1288, -0.0149,  ..., -0.0183, -0.0121, -0.0092],\n",
       "       device='cuda:0', dtype=torch.float16), 5: tensor([-0.0142,  0.2427,  0.0003,  ..., -0.0091,  0.0044, -0.0070],\n",
       "       device='cuda:0', dtype=torch.float16), 6: tensor([-0.0188,  0.2408, -0.0054,  ..., -0.0019, -0.0213,  0.0107],\n",
       "       device='cuda:0', dtype=torch.float16), 7: tensor([-0.0364,  0.1816,  0.1271,  ..., -0.0265, -0.0707,  0.0774],\n",
       "       device='cuda:0', dtype=torch.float16), 8: tensor([-0.0471, -0.2164,  0.3469,  ..., -0.1146, -0.1641, -0.0218],\n",
       "       device='cuda:0', dtype=torch.float16), 9: tensor([-0.0583, -0.6050,  0.0832,  ..., -0.1276, -0.2593, -0.1131],\n",
       "       device='cuda:0', dtype=torch.float16), 10: tensor([-0.0583, -0.6733,  0.0853,  ..., -0.2549, -0.2937, -0.0430],\n",
       "       device='cuda:0', dtype=torch.float16), 11: tensor([-0.1395, -0.8213,  0.1056,  ..., -0.2188, -0.6323, -0.0343],\n",
       "       device='cuda:0', dtype=torch.float16), 12: tensor([-0.1780, -1.0215,  0.0802,  ..., -0.2373, -0.2524, -0.2356],\n",
       "       device='cuda:0', dtype=torch.float16), 13: tensor([-0.2627, -1.5205,  0.7715,  ...,  0.3784,  0.3777,  0.3823],\n",
       "       device='cuda:0', dtype=torch.float16), 14: tensor([-0.2137, -1.6641,  1.1670,  ..., -0.2290,  1.3779, -0.4795],\n",
       "       device='cuda:0', dtype=torch.float16), 15: tensor([-0.4363, -0.7139,  0.9771,  ..., -0.1348, -0.2656,  0.8247],\n",
       "       device='cuda:0', dtype=torch.float16), 16: tensor([-0.6157,  0.4851,  1.5352,  ...,  1.4863, -0.4783,  1.2139],\n",
       "       device='cuda:0', dtype=torch.float16), 17: tensor([-4.4648,  0.6167,  1.4414,  ...,  2.3535, -3.6641,  0.7607],\n",
       "       device='cuda:0', dtype=torch.float16), 18: tensor([-6.0742, -1.2227,  1.0977,  ...,  5.0312, -4.0039,  4.2188],\n",
       "       device='cuda:0', dtype=torch.float16), 19: tensor([-9.8906, -0.7974,  1.1094,  ...,  5.6992,  1.1172,  6.6406],\n",
       "       device='cuda:0', dtype=torch.float16), 20: tensor([-9.6172, -1.4912,  0.4924,  ...,  3.6777, -0.0224,  4.6914],\n",
       "       device='cuda:0', dtype=torch.float16), 21: tensor([-9.9844, -0.5830,  0.2361,  ...,  1.2959, -2.7324,  4.2891],\n",
       "       device='cuda:0', dtype=torch.float16), 22: tensor([-16.0781,  -2.1680,   0.1791,  ...,  -0.3733,  -6.9531,   2.4414],\n",
       "       device='cuda:0', dtype=torch.float16), 23: tensor([-23.0781,  -2.3164,   0.4268,  ...,  -1.7900,  -5.6641,  -2.0508],\n",
       "       device='cuda:0', dtype=torch.float16), 24: tensor([-20.7188,  -1.3770,   0.2759,  ..., -11.8125,  -7.0820,  -3.8262],\n",
       "       device='cuda:0', dtype=torch.float16), 25: tensor([-15.0781,  -5.5859,  -1.4082,  ...,  -8.2422,  -3.2949,  -4.5859],\n",
       "       device='cuda:0', dtype=torch.float16), 26: tensor([ 13.4766,  -3.4941,  -0.5117,  ...,  -3.1836,  -3.3848, -18.2812],\n",
       "       device='cuda:0', dtype=torch.float16), 27: tensor([ 15.2969,  -3.5234,  -0.0402,  ...,  -5.7266,   1.7666, -15.3359],\n",
       "       device='cuda:0', dtype=torch.float16), 28: tensor([ 17.1719,   4.1680,   0.8887,  ..., -25.5469,  -6.1641,  -2.4941],\n",
       "       device='cuda:0', dtype=torch.float16), 29: tensor([ 29.1250,   1.4033,   0.1479,  ..., -23.6562, -10.2891,  -8.9062],\n",
       "       device='cuda:0', dtype=torch.float16), 30: tensor([ 37.1562,  -1.1084,   0.9731,  ..., -22.7656, -15.2422, -21.6719],\n",
       "       device='cuda:0', dtype=torch.float16), 31: tensor([ 31.8750,   1.1045,   0.6270,  ..., -32.8750,  -4.2383, -21.4844],\n",
       "       device='cuda:0', dtype=torch.float16), 32: tensor([ 35.5000, -11.8672,   0.6484,  ..., -41.0938, -21.5469, -24.2656],\n",
       "       device='cuda:0', dtype=torch.float16), 33: tensor([ 25.3125, -21.6719,   0.9795,  ..., -34.4062, -13.7188, -31.5312],\n",
       "       device='cuda:0', dtype=torch.float16), 34: tensor([ 47.0312, -33.6250,   0.4016,  ..., -43.1875, -10.8516, -32.6875],\n",
       "       device='cuda:0', dtype=torch.float16), 35: tensor([ 47.6250, -25.8438,   0.3396,  ..., -39.0938,  -2.6875, -32.3750],\n",
       "       device='cuda:0', dtype=torch.float16), 36: tensor([ 38.5000, -16.3125,   0.8608,  ..., -48.6875, -16.9844, -38.3125],\n",
       "       device='cuda:0', dtype=torch.float16), 37: tensor([ 41.2500, -15.3906,   0.5215,  ..., -41.6562, -26.0312, -50.7500],\n",
       "       device='cuda:0', dtype=torch.float16), 38: tensor([ 33.4062, -20.1875,   1.1113,  ..., -47.5312, -23.8750, -57.1562],\n",
       "       device='cuda:0', dtype=torch.float16), 39: tensor([ 41.4062,  -6.1445,   1.0752,  ..., -63.7188, -39.3750, -50.5625],\n",
       "       device='cuda:0', dtype=torch.float16), 40: tensor([ 23.1250,   2.8320,   0.8198,  ..., -77.0625, -34.5625, -41.7188],\n",
       "       device='cuda:0', dtype=torch.float16), 41: tensor([ 35.7188,   4.8711,   0.7344,  ..., -87.1250, -41.3438, -47.5938],\n",
       "       device='cuda:0', dtype=torch.float16), 42: tensor([ 52.5000,  -9.0938,   0.8838,  ..., -83.7500, -41.5625, -46.3438],\n",
       "       device='cuda:0', dtype=torch.float16), 43: tensor([ 60.9375,  -5.9922,   0.5845,  ..., -79.0625, -38.3438, -43.0625],\n",
       "       device='cuda:0', dtype=torch.float16), 44: tensor([ 80.5000,  -6.6562,   0.5747,  ..., -89.7500, -46.9375, -43.8438],\n",
       "       device='cuda:0', dtype=torch.float16), 45: tensor([ 70.3125,  -2.8105,   1.0850,  ..., -98.5625, -48.3438, -38.9375],\n",
       "       device='cuda:0', dtype=torch.float16), 46: tensor([  72.0625,   -1.2959,    1.6797,  ..., -105.4375,  -37.4375,\n",
       "         -34.4062], device='cuda:0', dtype=torch.float16), 47: tensor([ 81.0625,  -0.5439,  -3.4180,  ..., -90.5000, -22.8281, -37.5312],\n",
       "       device='cuda:0', dtype=torch.float16)}, layer_type='decoder_block')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8bd42a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear8bitLt(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear8bitLt(in_features=3840, out_features=4096, bias=False)\n",
       "            (k_proj): Linear8bitLt(in_features=3840, out_features=2048, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=3840, out_features=2048, bias=False)\n",
       "            (o_proj): Linear8bitLt(in_features=4096, out_features=3840, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=3840, out_features=15360, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=3840, out_features=15360, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=15360, out_features=3840, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3840, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7df68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, multiplier=0):\n",
    "    pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "    with steering_vector.apply(model, multiplier=multiplier, min_token_index=0):\n",
    "        result = pipeline(question, max_length=300, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "index = data[k][\"anthro\"].index(\"<start_of_turn>model\\n\")\n",
    "sample_question = data[k][\"anthro\"][:index+21]\n",
    "print(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9b30db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'handle' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/steering_vectors/steering_vector.py:142\u001b[39m, in \u001b[36mSteeringVector.apply\u001b[39m\u001b[34m(self, model, layer_config, operator, multiplier, min_token_index, token_indices)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     handle = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_token_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_token_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/steering_vectors/steering_vector.py:99\u001b[39m, in \u001b[36mSteeringVector.patch_activations\u001b[39m\u001b[34m(self, model, layer_config, operator, multiplier, min_token_index, token_indices)\u001b[39m\n\u001b[32m     97\u001b[39m layer_name = matching_layers[layer_num]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m target_activation = \u001b[43mmultiplier\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    101\u001b[39m module = get_module(model, layer_name)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(question, multiplier)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_response\u001b[39m(question, multiplier=\u001b[32m0\u001b[39m):\n\u001b[32m      2\u001b[39m     pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msteering_vector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_token_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/steering_vectors/steering_vector.py:152\u001b[39m, in \u001b[36mSteeringVector.apply\u001b[39m\u001b[34m(self, model, layer_config, operator, multiplier, min_token_index, token_indices)\u001b[39m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mhandle\u001b[49m.remove()\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'handle' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "print(generate_response(sample_question, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22417970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printBlue(f\"Question: {sample_question}\")\n",
    "# printBlue(\"\\nNon-anthropomorphic response (multiplier = -1):\")\n",
    "# printGreen(generate_response(sample_question, -1))\n",
    "# printBlue(\"\\nNeutral response (multiplier = 0):\")\n",
    "# printYellow(generate_response(sample_question, 0))\n",
    "# printBlue(\"\\nAnthropomorphic response (multiplier = 1):\")\n",
    "# printRed(generate_response(sample_question, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
